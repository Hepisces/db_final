import typer
import pandas as pd
import sqlite3
from contextlib import closing
import os
import random
from datetime import datetime, timedelta
import plotly.express as px
import sqlparse
import sys
import subprocess
import textwrap
from rich.console import Console
from rich.table import Table
from rich.tree import Tree
from rich.progress import track
from rich.syntax import Syntax
from pathlib import Path
from typer.models import OptionInfo

# Import refactored modules
from . import db
from . import visualizer
from . import checker
from . import text2sql

# --- Adapter for Python 3.12+ sqlite3 datetime handling ---
# This silences the DeprecationWarning by registering a modern adapter.
sqlite3.register_adapter(datetime, lambda val: val.isoformat())

app = typer.Typer(
    name="awesql",
    help="ä¸€ä¸ªç”¨äºæ•°æ®åº“äº¤äº’ã€æŸ¥è¯¢ã€å¯è§†åŒ–å’ŒAIè¾…åŠ©çš„å¼ºå¤§CLIå·¥å…·ã€‚",
    rich_markup_mode="markdown"
)
config_app = typer.Typer(name="config", help="ç®¡ç†AIåŠŸèƒ½çš„é…ç½®ï¼Œå¦‚æ¨¡å‹å’ŒDDLæ–‡ä»¶è·¯å¾„ã€‚")
app.add_typer(config_app)
console = Console()

DB_FILE = "visualization_demo.db"
DATA_DIR = "Smart_Home_DATA"
OUTPUT_DIR = Path("output")

PLAN_EXPLANATIONS = {
    "SCAN": "å…¨è¡¨æ‰«æ: ä»å¤´åˆ°å°¾è¯»å–è¡¨çš„æ¯ä¸€è¡Œã€‚å¯¹äºå¤§è¡¨ï¼Œè¿™å¯èƒ½æ•ˆç‡ä¸é«˜ã€‚é€šå¸¸æ„å‘³ç€æ²¡æœ‰ä½¿ç”¨ç´¢å¼•ã€‚",
    "SEARCH": "ç´¢å¼•æœç´¢: ä½¿ç”¨ç´¢å¼•æ¥ç›´æ¥å®šä½å’Œè¯»å–æ»¡è¶³æ¡ä»¶çš„è¡Œå­é›†ï¼Œè¿™é€šå¸¸æ¯”å…¨è¡¨æ‰«æå¿«å¾—å¤šã€‚",
    "USING INDEX": "ä½¿ç”¨äº†å‘½åç´¢å¼•ã€‚",
    "USING COVERING INDEX": "ä½¿ç”¨äº†è¦†ç›–ç´¢å¼•: æŸ¥è¯¢æ‰€éœ€çš„æ‰€æœ‰æ•°æ®éƒ½åŒ…å«åœ¨ç´¢å¼•ä¸­ï¼Œæ— éœ€è®¿é—®åŸå§‹è¡¨ï¼Œæ•ˆç‡éå¸¸é«˜ã€‚",
    "USE TEMP B-TREE FOR": "ä½¿ç”¨ä¸´æ—¶Bæ ‘: ä¸ºæ’åº(ORDER BY)æˆ–åˆ†ç»„(GROUP BY)åˆ›å»ºäº†ä¸€ä¸ªä¸´æ—¶å†…éƒ¨ç´¢å¼•ã€‚å¦‚æœé¢‘ç¹å‘ç”Ÿï¼Œå¯è€ƒè™‘ä¸ºç›¸å…³åˆ—æ·»åŠ çœŸå®ç´¢å¼•æ¥ä¼˜åŒ–ã€‚",
    "COMPOUND QUERY": "å¤åˆæŸ¥è¯¢: æ­£åœ¨æ‰§è¡Œä¸€ä¸ªåŒ…å« UNION, EXCEPT, æˆ– INTERSECT çš„æŸ¥è¯¢ã€‚",
    "MATERIALIZE": "ç‰©åŒ–å­æŸ¥è¯¢: å°†ä¸€ä¸ªå­æŸ¥è¯¢çš„ç»“æœå­˜å‚¨åœ¨ä¸€ä¸ªä¸´æ—¶è¡¨ä¸­ã€‚è¿™é€šå¸¸åœ¨å­æŸ¥è¯¢ç»“æœéœ€è¦è¢«å¤šæ¬¡ä½¿ç”¨æ—¶å‘ç”Ÿã€‚",
    "CO-ROUTINE": "ä½¿ç”¨åç¨‹: å°†å­æŸ¥è¯¢ä½œä¸ºåç¨‹è¿è¡Œï¼ŒæŒ‰éœ€ç”Ÿæˆè¡Œï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰ç»“æœã€‚è¿™å¯ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚"
}

def get_explanation(detail_str: str) -> str:
    """Finds a human-readable explanation for a query plan detail string."""
    for keyword, explanation in PLAN_EXPLANATIONS.items():
        if keyword in detail_str:
            return f" [dim italic]({explanation})[/dim italic]"
    return ""

# --- Database and Data Loading Logic ---

def create_connection():
    """Create a database connection to the SQLite database."""
    return sqlite3.connect(DB_FILE)

def create_tables(conn):
    """Create tables based on the DDL.sql file."""
    ddl_file_path = os.path.join(DATA_DIR, "DDL.sql")
    console.print(f"Reading table schema from [cyan]{ddl_file_path}[/cyan]...")
    try:
        with open(ddl_file_path, 'r', encoding='utf-8') as f:
            ddl_script = f.read()
        with closing(conn.cursor()) as cursor:
            cursor.executescript(ddl_script)
        conn.commit()
        console.print("[green]Tables created successfully.[/green]")
    except FileNotFoundError:
        console.print(f"[bold red]Error: DDL file not found at {ddl_file_path}. Cannot create tables.[/bold red]")
        raise
    except Exception as e:
        console.print(f"[bold red]An error occurred while creating tables: {e}[/bold red]")
        raise

def import_real_data(conn):
    """Import data from the real .sql files."""
    cursor = conn.cursor()
    
    # Order matters due to foreign keys
    sql_files = ['customer.sql', 'devlist.sql', 'control.sql', 'devupdata.sql']
    
    console.print(f"[yellow]Starting data import from [bold]{DATA_DIR}[/bold]...[/yellow]")
    
    for file_name in sql_files:
        file_path = os.path.join(DATA_DIR, file_name)
        console.print(f"Processing [cyan]{file_path}[/cyan]...")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                # Use a progress bar for large files
                for line in track(f, description=f"Importing {file_name}..."):
                    # Clean up the line and execute
                    sql_statement = line.strip()
                    if sql_statement:
                        # Remove the "public." schema prefix for SQLite compatibility
                        sql_statement = sql_statement.replace("INSERT INTO public.", "INSERT INTO ")
                        cursor.execute(sql_statement)
            conn.commit()
            console.print(f"[green]Successfully imported {file_name}.[/green]")
        except FileNotFoundError:
            console.print(f"[bold red]Error: File not found at {file_path}. Please check the path.[/bold red]")
            raise
        except Exception as e:
            console.print(f"[bold red]An error occurred while importing {file_name}: {e}[/bold red]")
            raise
            
    console.print("[bold green]All data has been successfully imported![/bold green]")

def db_exists():
    """Check if the database file exists and is not empty."""
    if not os.path.exists(DB_FILE) or os.path.getsize(DB_FILE) == 0:
        return False
    # Further check if tables are populated
    try:
        conn = create_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM customer")
        count = cursor.fetchone()[0]
        conn.close()
        return count > 0
    except sqlite3.Error:
        return False

# --- CLI Visualization Logic ---

def draw_query_plan(plan_df: pd.DataFrame):
    """Draw the query plan as a tree in the console using rich, with explanations."""
    console.print("\n[bold cyan]ğŸ“Š Query Plan[/bold cyan]")
    tree = Tree("[bold]Query Plan[/bold]")
    nodes = {}

    # First pass: add all nodes to the tree and dictionary
    for _, row in plan_df.iterrows():
        explanation = get_explanation(row['detail'])
        node_label = f"[green]ID:{row['id']}[/green] | {row['detail']}{explanation}"
        nodes[row['id']] = {'label': node_label, 'parent': row['parent'], 'children': []}

    # Second pass: build the tree structure
    root_nodes = []
    for node_id, node_data in nodes.items():
        parent_id = node_data['parent']
        if parent_id == 0:
            root_nodes.append(node_id)
        else:
            if parent_id in nodes:
                nodes[parent_id]['children'].append(node_id)

    # Recursive function to add nodes to the rich Tree
    def add_to_tree(tree_node, node_id):
        node_data = nodes[node_id]
        child_tree_node = tree_node.add(node_data['label'])
        for child_id in sorted(node_data['children']):
            add_to_tree(child_tree_node, child_id)

    # Build the tree from root nodes
    for root_id in sorted(root_nodes):
        add_to_tree(tree, root_id)
             
    console.print(tree)

def open_file(filepath):
    """Open a file in the default application for the current platform."""
    try:
        # ç¡®ä¿filepathæ˜¯å­—ç¬¦ä¸²
        filepath_str = str(filepath)
        
        if sys.platform == "win32":
            os.startfile(filepath_str)
        elif sys.platform == "darwin": # macOS
            subprocess.run(["open", filepath_str], check=True)
        else: # linux
            subprocess.run(["xdg-open", filepath_str], check=True)
        console.print(f"âœ… Automatically opened [bold white]{filepath_str}[/bold white]")
    except (FileNotFoundError, subprocess.CalledProcessError) as e:
        console.print(f"[bold red]Could not automatically open file:[/bold red] {e}")
        console.print(f"Please find it at: {os.path.abspath(filepath_str)}")

def print_results_table(df: pd.DataFrame):
    """Print the query results as a table in the console, limited to 10 rows."""
    console.print("\n[bold cyan]ğŸ“ˆ Query Results[/bold cyan]")
    if df.empty:
        console.print("[yellow]Query executed successfully, but returned no data.[/yellow]")
        return
    
    total_rows = len(df)
    display_df = df
    if total_rows > 10:
        console.print(f"[yellow]Total rows: {total_rows}. Showing top 10.[/yellow]")
        display_df = df.head(10)
        
    table = Table(show_header=True, header_style="bold magenta")
    for col in display_df.columns:
        table.add_column(col)
    
    for _, row in display_df.iterrows():
        table.add_row(*[str(item) for item in row])
        
    console.print(table)

def prepare_df_for_plotting(df: pd.DataFrame, category_col: str, metric_col: str, limit: int = 15) -> pd.DataFrame:
    """Limit the number of categories for plotting, grouping the rest into 'Other'."""
    if len(df) > limit:
        console.print(f"[yellow]Chart has {len(df)} categories. Displaying top {limit - 1} and aggregating the rest into 'Other'.[/yellow]")
        df_sorted = df.sort_values(by=metric_col, ascending=False)
        
        df_top = df_sorted.head(limit - 1)
        df_other = df_sorted.iloc[limit - 1:]
        
        if not df_other.empty:
            other_sum = df_other[metric_col].sum()
            other_row = pd.DataFrame([{category_col: 'Other', metric_col: other_sum}])
            df_plot = pd.concat([df_top, other_row], ignore_index=True)
        else:
            df_plot = df_top # Should not happen if len > limit but as a safeguard
            
        return df_plot
    return df

def try_convert_to_datetime(series: pd.Series) -> pd.Series | None:
    """
    Attempts to convert a pandas Series to datetime, trying multiple formats.
    Handles standard datetime strings and numeric unix timestamps (s, ms, us, ns).
    """
    # 1. Try standard conversion first (for ISO formats etc.)
    # Make a copy to avoid SettingWithCopyWarning
    series_copy = series.copy()
    try:
        # First try with a specific format
        converted_series = pd.to_datetime(series_copy, format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')
    except:
        # If that fails, fall back to automatic format detection
        converted_series = pd.to_datetime(series_copy, errors='coerce')
    
    if not pd.api.types.is_datetime64_any_dtype(converted_series) or converted_series.isnull().all():
        # 2. If it fails or results in all NaT, and dtype is numeric, try unix timestamp conversion
        if pd.api.types.is_numeric_dtype(series_copy):
            # Check for plausible unix timestamp in nanoseconds, microseconds, or milliseconds.
            # 1.6e18 (ns) is around 2020. A simple check for large numbers.
            # We check the mean to avoid issues with a few outlier points.
            if series_copy.mean() > 1e12: 
                # Attempt conversion from nanoseconds, most likely for IoT data
                converted_series = pd.to_datetime(series_copy, unit='ns', errors='coerce')
            else: # Otherwise, assume seconds
                converted_series = pd.to_datetime(series_copy, unit='s', errors='coerce')
    
    # Return the series only if the conversion was successful for at least one value
    if pd.api.types.is_datetime64_any_dtype(converted_series) and not converted_series.isnull().all():
        return converted_series
    
    return None

def visualize_and_save(query: str, df: pd.DataFrame, output_file: Path):
    """Infer query type, visualize results with a scientific style, and save to a file."""
    query_type_info = infer_query_type(query, df)
    query_type = query_type_info[0] if isinstance(query_type_info, tuple) else query_type_info
    
    console.print(f"\n[bold cyan]ğŸ–¼ï¸  Result Visualization[/bold cyan]")
    console.print(f"Inferred query type: [bold]{query_type}[/bold]")

    fig = None
    plot_template = "plotly_white"
    
    # Wrap the query and replace newlines with HTML breaks for Plotly
    chart_title = textwrap.fill(query, width=80).replace('\n', '<br>')

    if query_type == "æ—¶é—´åºåˆ—":
        _, time_col, value_col = query_type_info
        
        # --- Smart Data Conversion ---
        # 1. Convert time column to datetime and clean up
        df[time_col] = try_convert_to_datetime(df[time_col])
        df.dropna(subset=[time_col], inplace=True)
        df = df.sort_values(by=time_col)

        # 2. Attempt to convert value column to numeric
        value_series_numeric = pd.to_numeric(df[value_col], errors='coerce')

        # 3. Decide plotting strategy based on value column type
        if not value_series_numeric.isnull().all(): # Case 1: Value column is NUMERIC
            console.print(f"Plotting numeric time series: [cyan]'{time_col}'[/cyan] vs [cyan]'{value_col}'[/cyan].")
            df['numeric_value'] = value_series_numeric
            df_plot = df.dropna(subset=['numeric_value'])
            if not df_plot.empty:
                fig = px.line(df_plot, x=time_col, y='numeric_value', title=chart_title, markers=True, template=plot_template)
                fig.update_yaxes(title_text=value_col) # Use original column name for y-axis title
                
                # Ensure X-axis shows proper datetime format
                fig.update_xaxes(
                    type='date',
                    tickformat='%Y-%m-%d %H:%M:%S',
                    title_text=time_col
                )
        
        else: # Case 2: Value column is CATEGORICAL
            console.print(f"Plotting categorical time series: [cyan]'{time_col}'[/cyan] vs [cyan]'{value_col}'[/cyan].")
            df_plot = df.dropna(subset=[value_col])

            if not df_plot.empty:
                # Create a mapping from category to integer
                categories = df_plot[value_col].astype(str).unique()
                category_map = {cat: i for i, cat in enumerate(categories)}
                
                # Apply the mapping to create a numeric column for plotting
                df_plot['numeric_value'] = df_plot[value_col].map(category_map)
                
                # Use a step chart (line_shape='hv') for clearer state transitions
                fig = px.line(df_plot, x=time_col, y='numeric_value', title=chart_title, markers=True, template=plot_template, line_shape='hv')
                
                # Update the y-axis to show the original string labels instead of numbers
                fig.update_yaxes(
                    tickvals=list(category_map.values()),
                    ticktext=list(category_map.keys()),
                    title_text=value_col
                )
                
                # Ensure X-axis shows proper datetime format
                fig.update_xaxes(
                    type='date',
                    tickformat='%Y-%m-%d %H:%M:%S',
                    title_text=time_col
                )

    elif query_type in ["èšåˆåˆ†æ", "ç±»åˆ«åˆ†å¸ƒ"]:
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        categorical_cols = df.select_dtypes(exclude=['number']).columns.tolist()
        if numeric_cols and categorical_cols:
            category_col = categorical_cols[0]
            metric_col = numeric_cols[0]
            df_plot = prepare_df_for_plotting(df, category_col, metric_col)
            fig = px.bar(df_plot, x=category_col, y=metric_col, title=chart_title, template=plot_template)

    if fig:
        fig.update_layout(
            title_font_size=16,
            font=dict(family="Arial, sans-serif", size=14, color="black"),
            xaxis_title_font_size=16,
            yaxis_title_font_size=16,
        )
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ï¼ˆè™½ç„¶åº”è¯¥å·²ç»åœ¨è°ƒç”¨å‰åˆ›å»ºäº†ï¼‰
            OUTPUT_DIR.mkdir(exist_ok=True)
            
            # å°†Pathå¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥ä¾›plotlyä½¿ç”¨
            output_path = str(output_file)
            
            fig.write_image(output_path, scale=2) # Increase scale for better resolution
            console.print(f"[green]âœ… Visualization saved to [bold white]{output_path}[/bold white][/green]")
            open_file(output_path)
        except Exception as e:
            console.print(f"[red]Error saving visualization: {e}[/red]")
            console.print("[yellow]Please ensure you have the 'kaleido' package installed (`pip install kaleido`).[/yellow]")
    else:
        console.print("[yellow]No suitable visualization generated for this query type.[/yellow]")

def infer_query_type(query, df):
    """
    Infer query type to select a suitable visualization method.
    This version is more robust and attempts to parse date-like strings and numeric timestamps.
    Returns a tuple: (query_type, col1_name, col2_name) for plottable types.
    """
    formatted_query = sqlparse.format(query.strip(), keyword_case='upper')
    
    # --- 1. First check for Aggregation/Categorical queries by SQL syntax ---
    # This takes precedence over time series detection
    has_aggregation = ('GROUP BY' in formatted_query or 
                      any(f in formatted_query for f in ['COUNT(', 'SUM(', 'AVG(', 'MIN(', 'MAX(']))
    
    if has_aggregation:
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        categorical_cols = df.select_dtypes(exclude=['number']).columns.tolist()
        
        if numeric_cols and categorical_cols:
            console.print("Query contains GROUP BY or aggregation functions. Treating as aggregation analysis.")
            return "èšåˆåˆ†æ"
    
    # --- 2. Check for Time Series ---
    # Skip columns that are clearly aggregation results
    skip_columns = [col for col in df.columns if any(agg in col.lower() for agg in 
                   ['count(', 'sum(', 'avg(', 'min(', 'max(', 'count', 'sum', 'avg'])]
    
    # Find a time column
    time_col_name = None
    for col_name in df.columns:
        # Skip aggregation result columns for time detection
        if col_name in skip_columns:
            continue
            
        if try_convert_to_datetime(df[col_name]) is not None:
            time_col_name = col_name
            break
            
    if time_col_name:
        # Found a time column. Find a suitable value column that is not the time column.
        value_cols = [col for col in df.columns if col != time_col_name]
        if value_cols:
            console.print(f"Found time column: '{time_col_name}'. Treating as time series.")
            return "æ—¶é—´åºåˆ—", time_col_name, value_cols[0]

    # --- 3. Check for simple categorical distributions ---
    if len(df.columns) == 2 and pd.api.types.is_numeric_dtype(df.iloc[:, 1]):
        return "ç±»åˆ«åˆ†å¸ƒ"

    # --- 4. Default to general query ---
    return "ä¸€èˆ¬æŸ¥è¯¢"

def _is_admin(username, password):
    """
    æ£€æŸ¥ç®¡ç†å‘˜å‡­æ®ã€‚
    ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå¦‚æœæœªè®¾ç½®åˆ™å›é€€åˆ°é»˜è®¤å€¼ã€‚
    """
    admin_user = os.environ.get("AWESQL_ADMIN_USER", "admin")
    admin_pass = os.environ.get("AWESQL_ADMIN_PASS", "123")
    if username == admin_user and password == admin_pass:
        return True
    console.print("[bold red]è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ç”¨æˆ·åå’Œå¯†ç ã€‚[/bold red]")
    return False

@app.command()
def run(
    query: str = typer.Argument(..., help="è¦æ‰§è¡Œå’Œå¯è§†åŒ–çš„SQLæŸ¥è¯¢ã€‚"),
    username: str = typer.Option(None, "--user", "-u", help="æ‰§è¡Œä¿®æ”¹æ€§æŸ¥è¯¢æ‰€éœ€çš„ç®¡ç†å‘˜ç”¨æˆ·åã€‚"),
    password: str = typer.Option(None, "--pass", "-p", help="æ‰§è¡Œä¿®æ”¹æ€§æŸ¥è¯¢æ‰€éœ€çš„ç®¡ç†å‘˜å¯†ç ã€‚", hide_input=True),
    db_name: str = typer.Option("project2025.db", "--db-name", help="è¦æŸ¥è¯¢çš„æ•°æ®åº“æ–‡ä»¶çš„åç§°ã€‚")
):
    """
    æ‰§è¡ŒSQLæŸ¥è¯¢ï¼Œæ˜¾ç¤ºç»“æœã€æŸ¥è¯¢è®¡åˆ’ï¼Œå¹¶ä¸ºSELECTæŸ¥è¯¢ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ã€‚

    - **å¯¹äºSELECTæŸ¥è¯¢**: åŒæ—¶æ˜¾ç¤ºæŸ¥è¯¢è®¡åˆ’ã€ç»“æœè¡¨æ ¼ï¼Œå¹¶ç”Ÿæˆå›¾è¡¨ã€‚
    - **å¯¹äºä¿®æ”¹æ€§æŸ¥è¯¢ (INSERT, UPDATE, DELETE)**: éœ€è¦ç®¡ç†å‘˜æƒé™ (`--user`, `--pass`)ã€‚
    """
    if not db.db_exists(db_name):
        console.print(f"[bold yellow]è­¦å‘Š: æ•°æ®åº“ '{db_name}' ä¸å­˜åœ¨æˆ–ä¸ºç©ºã€‚[/bold yellow]")
        console.print("è¯·å…ˆä½¿ç”¨ `import-data` å‘½ä»¤å¯¼å…¥æ•°æ®ã€‚")
        raise typer.Exit()

    # Sanitize query by removing trailing semicolons
    query = query.strip().rstrip(';')

    # Check if it's a read-only query
    is_read_only = is_read_only_query(query)

    if not is_read_only:
        console.print("[bold yellow]è­¦å‘Š: è¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªä¿®æ”¹æ€§æŸ¥è¯¢ã€‚[/bold yellow]")
        if not _is_admin(username, password):
            console.print("[bold red]é”™è¯¯: ä¿®æ”¹æ€§æŸ¥è¯¢éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚è¯·æä¾›æ­£ç¡®çš„ --user å’Œ --passã€‚[/bold red]")
            raise typer.Exit(code=1)
        console.print("[green]ç®¡ç†å‘˜æƒé™éªŒè¯é€šè¿‡ã€‚[/green]")

    console.print(f"æ­£åœ¨æ‰§è¡ŒæŸ¥è¯¢: [magenta]'{query}'[/magenta]...")

    # For non-select queries, we just execute them
    if not query.lower().strip().startswith("select"):
        try:
            with db.create_connection(db_name) as conn:
                conn.execute(query)
                conn.commit()
                changes = conn.total_changes
                console.print(f"[green]æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œå½±å“äº† {changes} è¡Œã€‚[/green]")
        except sqlite3.Error as e:
            console.print(f"[bold red]æ•°æ®åº“é”™è¯¯: {e}[/bold red]")
        return

    # For SELECT queries, proceed with visualization
    result_df, plan_df = db.execute_query(query, db_name=db_name)

    if result_df is None:
        console.print("[bold red]æŸ¥è¯¢æ‰§è¡Œå¤±è´¥ï¼Œæ— æ³•ç»§ç»­ã€‚[/bold red]")
        raise typer.Exit(code=1)
        
    # 1. Draw Query Plan
    if plan_df is not None and not plan_df.empty:
        visualizer.draw_query_plan(plan_df)
    else:
        console.print("[yellow]æœªèƒ½è·å–æŸ¥è¯¢è®¡åˆ’ã€‚[/yellow]")

    # 2. Print Results Table
    visualizer.print_results_table(result_df)
        
    # 3. Visualize and Save Chart using the interactive visualizer
    if not result_df.empty:
        # Create a safe filename from the query
        safe_filename = "".join(c if c.isalnum() else "_" for c in query)[:50]
        # ä½¿ç”¨Pathå¯¹è±¡æ­£ç¡®æ‹¼æ¥è·¯å¾„
        output_file = OUTPUT_DIR / f"{safe_filename}.png"
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        OUTPUT_DIR.mkdir(exist_ok=True)

        # Call the interactive visualizer from the visualizer module
        visualizer.visualize_query_result(result_df, query, output_file)
    else:
        console.print("[yellow]æŸ¥è¯¢æœªè¿”å›æ•°æ®ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆã€‚[/yellow]")

@app.command()
def tables(
    db_name: str = typer.Option("project2025.db", "--db-name", help="è¦æ£€æŸ¥çš„æ•°æ®åº“æ–‡ä»¶çš„åç§°ã€‚")
):
    """
    åˆ—å‡ºæ•°æ®åº“ä¸­æ‰€æœ‰çš„è¡¨ã€‚
    """
    if not db.db_exists(db_name):
        console.print(f"[bold yellow]è­¦å‘Š: æ•°æ®åº“ '{db_name}' ä¸å­˜åœ¨æˆ–ä¸ºç©ºã€‚[/bold yellow]")
        console.print("è¯·å…ˆä½¿ç”¨ `import-data` å‘½ä»¤å¯¼å…¥æ•°æ®ã€‚")
        return

    console.print(f"æ­£åœ¨ä» [cyan]{db_name}[/cyan] è·å–è¡¨åˆ—è¡¨...")
    table_names = db.get_table_names(db_name)

    if table_names is None:
        console.print("[bold red]æ— æ³•æ£€ç´¢åˆ°è¡¨ã€‚[/bold red]")
        return
    
    if not table_names:
        console.print("[yellow]æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡¨ã€‚[/yellow]")
        return
        
    table = Table(title=f"æ•°æ®åº“ '{db_name}' ä¸­çš„è¡¨", show_header=True, header_style="bold magenta")
    table.add_column("åºå·", style="dim", width=6)
    table.add_column("è¡¨å")

    for i, name in enumerate(table_names):
        table.add_row(str(i + 1), name)
    
    console.print(table)

@app.command()
def er(
    data_dir: Path = typer.Option(
        "Smart_Home_DATA",
        "--dir",
        "-d",
        help="åŒ…å«DDL.sqlæ–‡ä»¶çš„ç›®å½•ã€‚",
        exists=True,
        file_okay=False,
        dir_okay=True,
        resolve_path=True,
    ),
):
    """
    ä» DDL æ–‡ä»¶ç”Ÿæˆå¹¶æ˜¾ç¤ºæ•°æ®åº“çš„ E-R (å®ä½“-å…³ç³») å›¾ã€‚
    """
    ddl_path = data_dir / "DDL.sql"
    output_path = OUTPUT_DIR / "er_diagram.html"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    OUTPUT_DIR.mkdir(exist_ok=True)
    
    console.print(f"æ­£åœ¨ä» [cyan]{ddl_path}[/cyan] ç”Ÿæˆ E-R å›¾...")
    visualizer.generate_er_diagram(str(ddl_path), str(output_path))

@app.command()
def import_data(
    data_dir: Path = typer.Option(
        "Smart_Home_DATA", 
        "--dir", 
        "-d", 
        help="åŒ…å«DDL.sqlå’Œæ•°æ®æ–‡ä»¶çš„ç›®å½•ã€‚é»˜è®¤ä¸º 'Smart_Home_DATA'ã€‚",
        exists=True,
        file_okay=False,
        dir_okay=True,
        resolve_path=True,
    ),
    db_name: str = typer.Option("project2025.db", "--db-name", help="ä¸ºæ•°æ®åº“æŒ‡å®šä¸€ä¸ªè‡ªå®šä¹‰åç§°ã€‚")
):
    """
    å°†SQLæ–‡ä»¶ä¸­çš„æ•°æ®å¯¼å…¥æ–°æ•°æ®åº“ã€‚
    æ­¤å‘½ä»¤åœ¨å¯¼å…¥å‰ä¼šé‡ç½®æ•°æ®åº“ã€‚
    """
    db.reset_db(db_name)
    
    try:
        with db.create_connection(db_name) as conn:
            if not db.create_tables(conn, str(data_dir)):
                console.print("[bold red]åˆ›å»ºæ•°æ®åº“è¡¨å¤±è´¥ï¼Œå¯¼å…¥ä¸­æ­¢ã€‚[/bold red]")
                return
            db.import_real_data(conn, str(data_dir))
        console.print(f"[bold green]æ‰€æœ‰æ•°æ®å·²æˆåŠŸå¯¼å…¥åˆ° '{db_name}'ã€‚[/bold green]")
    except Exception as e:
        console.print(f"[bold red]æ•°æ®å¯¼å…¥æœŸé—´å‡ºé”™: {e}[/bold red]")

@app.command()
def reset_db(
    username: str = typer.Option(..., "--user", "-u", help="ç®¡ç†å‘˜ç”¨æˆ·åã€‚", prompt=True),
    password: str = typer.Option(..., "--pass", "-p", help="ç®¡ç†å‘˜å¯†ç ã€‚", prompt=True, hide_input=True),
    db_name: str = typer.Option("project2025.db", "--db-name", help="è¦åˆ é™¤çš„æ•°æ®åº“æ–‡ä»¶çš„åç§°ã€‚")
):
    """(ä»…é™ç®¡ç†å‘˜) åˆ é™¤ç°æœ‰æ•°æ®åº“æ–‡ä»¶ä»¥é‡æ–°å¼€å§‹ã€‚"""
    if not _is_admin(username, password):
        return
    try:
        db.reset_db(db_name)
        # æˆåŠŸåˆ é™¤æ•°æ®åº“åï¼Œè¯¢é—®æ˜¯å¦åˆ é™¤é…ç½®æ–‡ä»¶
        if os.path.exists(db.CONFIG_FILE):
            delete_config = typer.confirm(
                "æ•°æ®åº“å·²åˆ é™¤ã€‚æ‚¨æ˜¯å¦ä¹Ÿæƒ³åˆ é™¤é…ç½®æ–‡ä»¶ 'awesql_config.json'ï¼Ÿ"
            )
            if delete_config:
                db.reset_config()
    except Exception as e:
        console.print(f"[bold red]é‡ç½®æ•°æ®åº“æ—¶å‡ºé”™: {e}[/bold red]")

@app.command()
def check(
    query: str = typer.Argument(..., help="è¦æ£€æŸ¥æ­£ç¡®æ€§çš„SQLæŸ¥è¯¢ã€‚")
):
    """ä½¿ç”¨AIåŠ©æ‰‹æ£€æŸ¥æ‰€æä¾›SQLæŸ¥è¯¢çš„æ­£ç¡®æ€§ã€‚"""
    console.print("æ­£åœ¨æ£€æŸ¥SQLæŸ¥è¯¢...")
    try:
        config = db.load_config()
        ddl_path = config.get("ddl_path")

        schema_content = ""
        if ddl_path and os.path.exists(ddl_path):
            with open(ddl_path, 'r', encoding='utf-8') as f:
                schema_content = f.read()
        
        result = checker.check_sql_query(query, schema_content)
        console.print(f"\n[bold green]AIæ£€æŸ¥ç»“æœ:[/bold green]\n{result}")
    except Exception as e:
        console.print(f"[bold red]SQLæŸ¥è¯¢æ£€æŸ¥å¤±è´¥: {e}[/bold red]")

@app.command()
def ask(
    question: str = typer.Argument(..., help="è¦è½¬æ¢ä¸ºSQLçš„è‡ªç„¶è¯­è¨€é—®é¢˜ã€‚")
):
    """å°†è‡ªç„¶è¯­è¨€é—®é¢˜ç¿»è¯‘æˆSQLæŸ¥è¯¢ã€‚"""
    console.print(f"æ­£åœ¨æ ¹æ®æ‚¨çš„é—®é¢˜ç”ŸæˆSQL: \"{question}\"")
    try:
        config = db.load_config()
        model_path = config.get("model_path")
        ddl_path = config.get("ddl_path")

        if not model_path or not ddl_path:
            console.print("[bold red]é”™è¯¯: æ¨¡å‹è·¯å¾„æˆ–DDLè·¯å¾„æœªé…ç½®ã€‚[/bold red]")
            console.print("è¯·è¿è¡Œ [bold]'awesql config set-model-path'[/bold] å’Œ [bold]'awesql config set-ddl-path'[/bold]è¿›è¡Œè®¾ç½®ã€‚")
            return

        sql_query = text2sql.generate_sql(question, ddl_path, model_path)
        
        if not sql_query:
            console.print("[bold red]æ— æ³•ç”ŸæˆSQLæŸ¥è¯¢ã€‚è¯·æ£€æŸ¥æ¨¡å‹æˆ–é—®é¢˜ã€‚[/bold red]")
            return
            
        console.print("\n[bold green]ğŸ‰ ç”Ÿæˆçš„SQLæŸ¥è¯¢:[/bold green]")
        syntax = Syntax(sql_query, "sql", theme="github-dark", line_numbers=True)
        console.print(syntax)

        if typer.confirm("æ˜¯å¦è¦å¯¹æ­¤SQLè¿›è¡Œæ£€æŸ¥?"):
            check(query=sql_query)
            
        # if typer.confirm("æ‚¨æƒ³ç«‹å³æ‰§è¡Œæ­¤æŸ¥è¯¢å—?"):
        #     run(query=sql_query)
        console.print(f"ç”ŸæˆæˆåŠŸ, å¯ä»¥ä½¿ç”¨[bold]'awesql run'[/bold]æ‰§è¡Œæ­¤æŸ¥è¯¢")


    except Exception as e:
        console.print(f"[bold red]æ–‡æœ¬åˆ°SQLè½¬æ¢æœŸé—´å‡ºé”™: {e}[/bold red]")

@config_app.command(name="set-model-path")
def set_model_path(
    path: Path = typer.Argument(
        ..., 
        help="æœ¬åœ°HuggingFaceæ¨¡å‹ç›®å½•çš„ç»å¯¹è·¯å¾„ã€‚",
        exists=True, 
        file_okay=False, 
        dir_okay=True,
        resolve_path=True
    )
):
    """Sets and saves the local model path for the 'ask' command."""
    config = db.load_config()
    config["model_path"] = str(path)
    db.save_config(config)
    console.print(f"[green]æ¨¡å‹è·¯å¾„å·²è®¾ä¸º: [cyan]{path}[/cyan][/green]")

@config_app.command(name="set-ddl-path")
def set_ddl_path(
    path: Path = typer.Argument(
        ..., 
        help="ç”¨äºAIè¾…åŠ©çš„DDL.sqlæ–‡ä»¶çš„ç»å¯¹è·¯å¾„ã€‚",
        exists=True,
        file_okay=True,
        dir_okay=False,
        resolve_path=True,
    )
):
    """Sets and saves the DDL file path for AI-powered commands."""
    config = db.load_config()
    config["ddl_path"] = str(path)
    db.save_config(config)
    console.print(f"[green]DDLæ–‡ä»¶è·¯å¾„å·²è®¾ä¸º: [cyan]{path}[/cyan][/green]")

@config_app.command(name="show")
def show_config():
    """Displays the current configuration."""
    config = db.load_config()
    if not config:
        console.print("[yellow]æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ã€‚è¯·ä½¿ç”¨ `set-model-path` æˆ– `set-ddl-path` åˆ›å»ºã€‚[/yellow]")
        return
    
    console.print("[bold]å½“å‰é…ç½®:[/bold]")
    for key, value in config.items():
        console.print(f"  [cyan]{key}[/cyan]: {value}")

# --- Utility Functions ---
def is_read_only_query(query: str) -> bool:
    """
    ä½¿ç”¨ sqlparse æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦åªåŒ…å« SELECT è¯­å¥ã€‚
    """
    parsed = sqlparse.parse(query)
    for statement in parsed:
        # get_type() è¿”å›è¯­å¥çš„ç±»å‹: 'SELECT', 'INSERT', 'UPDATE', 'DELETE', 'UNKNOWN'
        if statement.get_type() != 'SELECT':
            return False
    return True

if __name__ == "__main__":
    app() 