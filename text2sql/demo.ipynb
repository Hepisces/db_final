{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# å¿½ç•¥ç‰¹å®šçš„è­¦å‘Š\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*Torch was not compiled with flash attention.*\"\n",
    ")\n",
    "\n",
    "\n",
    "def load_sqlcoder_fixed():\n",
    "    \"\"\"\n",
    "    ä¿®å¤ç‰ˆæœ¬çš„SQLCoderæ¨¡å‹åŠ è½½ï¼Œä½¿ç”¨è‡ªå®šä¹‰ä¸‹è½½è·¯å¾„\n",
    "    \"\"\"\n",
    "    print(\"æ­£åœ¨åŠ è½½SQLCoderæ¨¡å‹...\")\n",
    "\n",
    "    try:\n",
    "        # è®¾ç½®è‡ªå®šä¹‰ä¸‹è½½è·¯å¾„\n",
    "        model_path = r\"models--defog--sqlcoder-7b-2/snapshots/7e5b6f7981c0aa7d143f6bec6fa26625bdfcbe66\"\n",
    "        print(f\"ä»æœ¬åœ°è·¯å¾„ {model_path} åŠ è½½æ¨¡å‹...\")\n",
    "\n",
    "        # ä¿®æ”¹tokenizeré…ç½®\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path, trust_remote_code=True, local_files_only=True\n",
    "        )\n",
    "\n",
    "        # æ˜¾å¼è®¾ç½®pad_tokenå’Œpad_token_id\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        # ä¼˜åŒ–æ¨¡å‹åŠ è½½é…ç½®\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            local_files_only=True,\n",
    "            use_flash_attention_2=False,  # ç¦ç”¨flash attention\n",
    "            do_sample=False,\n",
    "            num_beams=4\n",
    "        )\n",
    "\n",
    "        # ç¡®ä¿æ¨¡å‹çš„pad_token_idä¸tokenizerä¸€è‡´\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "        model.eval()\n",
    "\n",
    "        print(\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!\")\n",
    "        return tokenizer, model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def generate_sql_sqlcoder(sql_coder_tokenizer, sql_coder_model, question, schema=\"\"):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨SQLCoderç”ŸæˆSQLï¼Œä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹\n",
    "\n",
    "    Args:\n",
    "        question: è‡ªç„¶è¯­è¨€é—®é¢˜\n",
    "        schema: æ•°æ®åº“è¡¨ç»“æ„ï¼ˆå¯é€‰ï¼‰\n",
    "    \"\"\"\n",
    "    if sql_coder_tokenizer is None or sql_coder_model is None:\n",
    "        return \"æ¨¡å‹æœªæ­£ç¡®åŠ è½½\"\n",
    "\n",
    "    # æ„å»ºprompt\n",
    "    if schema:\n",
    "        prompt = f\"\"\"\n",
    "### Task\n",
    "Generate a SQL query to answer [QUESTION]{question}[/QUESTION]\n",
    "\n",
    "### Database Schema\n",
    "The query will run on a database with the following schema:\n",
    "{schema}\n",
    "\n",
    "### Answer\n",
    "Given the database schema, here is the SQL query that [QUESTION]{question}[/QUESTION]\n",
    "[SQL]\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "### Task\n",
    "Generate a SQL query to answer [QUESTION]{question}[/QUESTION]\n",
    "\n",
    "### Answer\n",
    "Here is the SQL query that [QUESTION]{question}[/QUESTION]\n",
    "[SQL]\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # ç¼–ç è¾“å…¥å¹¶æ·»åŠ attention mask\n",
    "        inputs = sql_coder_tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,  # ä¿æŒpaddingä¸ºFalse\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        # å°†è¾“å…¥ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡\n",
    "        device = next(sql_coder_model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # ç”ŸæˆSQL\n",
    "        with torch.no_grad(), torch.amp.autocast(\n",
    "            device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        ):  # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦\n",
    "            outputs = sql_coder_model.generate(\n",
    "                **inputs,\n",
    "                do_sample=False,\n",
    "                num_beams=4\n",
    "            )\n",
    "\n",
    "        # è§£ç ç»“æœ\n",
    "        full_response = sql_coder_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # æå–SQLéƒ¨åˆ†\n",
    "        if \"### SQL\" in full_response:\n",
    "            sql = full_response.split(\"### SQL\")[-1].strip()\n",
    "        else:\n",
    "            sql = full_response[len(prompt) :].strip()\n",
    "\n",
    "        return sql\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ç”ŸæˆSQLæ—¶å‡ºé”™: {e}\")\n",
    "        return f\"ç”Ÿæˆå¤±è´¥: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åŠ è½½SQLCoderæ¨¡å‹...\n",
      "ä»æœ¬åœ°è·¯å¾„ models--defog--sqlcoder-7b-2/snapshots/7e5b6f7981c0aa7d143f6bec6fa26625bdfcbe66 åŠ è½½æ¨¡å‹...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90cb33a5b5440ce9e5d2c527ac8ecfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ æ¨¡å‹åŠ è½½å¤±è´¥: Invalid buffer size: 12.55 GB\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ¨¡å‹\n",
    "sql_coder_tokenizer, sql_coder_model = load_sqlcoder_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     13\u001b[39m     schema = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33mCreate table If Not Exists Customers (id int, name varchar(255))\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33mCreate table If Not Exists Orders (id int, customerId int)\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m \u001b[33minsert into Orders (id, customerId) values (\u001b[39m\u001b[33m'\u001b[39m\u001b[33m2\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     26\u001b[39m     test_q2 = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[33mæ‰¾å‡ºæ‰€æœ‰ä»ä¸ç‚¹ä»»ä½•ä¸œè¥¿çš„é¡¾å®¢ã€‚\u001b[39m\n\u001b[32m     28\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m \u001b[33m+-----------+\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     result2 = \u001b[43mgenerate_sql_sqlcoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43msql_coder_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_coder_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_q2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mé—®é¢˜: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_q2\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSQL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult2\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 114\u001b[39m, in \u001b[36mgenerate_sql_sqlcoder\u001b[39m\u001b[34m(sql_coder_tokenizer, sql_coder_model, question, schema)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# ç”ŸæˆSQL\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(), torch.amp.autocast(\n\u001b[32m    112\u001b[39m     device_type=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    113\u001b[39m ):  \u001b[38;5;66;03m# ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     outputs = \u001b[43msql_coder_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_coder_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_coder_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# å¯ç”¨KVç¼“å­˜\u001b[39;49;00m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# è§£ç ç»“æœ\u001b[39;00m\n\u001b[32m    127\u001b[39m full_response = sql_coder_tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/generation/utils.py:2616\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n\u001b[32m   2615\u001b[39m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2616\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2617\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2621\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2622\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2623\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2625\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2626\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2627\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2628\u001b[39m         batch_size=batch_size,\n\u001b[32m   2629\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2635\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2636\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/generation/utils.py:4030\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   4027\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_attentions\u001b[39m\u001b[33m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   4028\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m-> \u001b[39m\u001b[32m4030\u001b[39m model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   4032\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   4033\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   4034\u001b[39m     model_outputs,\n\u001b[32m   4035\u001b[39m     model_kwargs,\n\u001b[32m   4036\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   4037\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    683\u001b[39m output_hidden_states = (\n\u001b[32m    684\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    685\u001b[39m )\n\u001b[32m    687\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    701\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:443\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    440\u001b[39m hidden_states = inputs_embeds\n\u001b[32m    442\u001b[39m \u001b[38;5;66;03m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m position_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrotary_emb\u001b[49m(hidden_states, position_ids)\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# decoder layers\u001b[39;00m\n\u001b[32m    446\u001b[39m all_hidden_states = () \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1927\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1922\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[32m   1926\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1927\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Union[Tensor, \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1928\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1929\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•\n",
    "if sql_coder_tokenizer and sql_coder_model:\n",
    "    # print(\"\\nğŸ” SQLCoderæµ‹è¯•:\")\n",
    "\n",
    "    # # ç®€å•æµ‹è¯•\n",
    "    # test_q = \"æ‰¾å‡ºæ‰€æœ‰å‘˜å·¥çš„å§“åå’Œè–ªæ°´\"\n",
    "    # result = generate_sql_sqlcoder(test_q)\n",
    "    # print(f\"é—®é¢˜: {test_q}\")\n",
    "    # print(f\"SQL: {result}\")\n",
    "    # print(\"-\" * 50)\n",
    "\n",
    "    # å¸¦è¡¨ç»“æ„çš„æµ‹è¯•\n",
    "    schema = \"\"\"\n",
    "Create table If Not Exists Customers (id int, name varchar(255))\n",
    "Create table If Not Exists Orders (id int, customerId int)\n",
    "Truncate table Customers\n",
    "insert into Customers (id, name) values ('1', 'Joe')\n",
    "insert into Customers (id, name) values ('2', 'Henry')\n",
    "insert into Customers (id, name) values ('3', 'Sam')\n",
    "insert into Customers (id, name) values ('4', 'Max')\n",
    "Truncate table Orders\n",
    "insert into Orders (id, customerId) values ('1', '3')\n",
    "insert into Orders (id, customerId) values ('2', '1')\n",
    "    \"\"\"\n",
    "\n",
    "    test_q2 = \"\"\"\n",
    "æ‰¾å‡ºæ‰€æœ‰ä»ä¸ç‚¹ä»»ä½•ä¸œè¥¿çš„é¡¾å®¢ã€‚\n",
    "\n",
    "ä»¥ ä»»æ„é¡ºåº è¿”å›ç»“æœè¡¨ã€‚\n",
    "\n",
    "ç»“æœæ ¼å¼å¦‚ä¸‹æ‰€ç¤ºã€‚\n",
    "\n",
    "\n",
    "\n",
    "ç¤ºä¾‹ 1ï¼š\n",
    "\n",
    "è¾“å…¥ï¼š\n",
    "Customers table:\n",
    "+----+-------+\n",
    "| id | name  |\n",
    "+----+-------+\n",
    "| 1  | Joe   |\n",
    "| 2  | Henry |\n",
    "| 3  | Sam   |\n",
    "| 4  | Max   |\n",
    "+----+-------+\n",
    "Orders table:\n",
    "+----+------------+\n",
    "| id | customerId |\n",
    "+----+------------+\n",
    "| 1  | 3          |\n",
    "| 2  | 1          |\n",
    "+----+------------+\n",
    "è¾“å‡ºï¼š\n",
    "+-----------+\n",
    "| Customers |\n",
    "+-----------+\n",
    "| Henry     |\n",
    "| Max       |\n",
    "+-----------+\n",
    "\"\"\"\n",
    "    result2 = generate_sql_sqlcoder(\n",
    "        sql_coder_tokenizer, sql_coder_model, test_q2, schema\n",
    "    )\n",
    "    print(f\"é—®é¢˜: {test_q2}\")\n",
    "    print(f\"SQL: {result2}\")\n",
    "else:\n",
    "    print(\"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
